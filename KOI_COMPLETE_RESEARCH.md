# KOI Infrastructure for GAIA AI: Complete Research & Implementation Plan

## Executive Summary

After extensive research into KOI (Knowledge Organization Infrastructure) and iterative refinement based on architectural feedback, we propose a phased implementation that transforms GAIA AI from a basic RAG system into a sophisticated, distributed knowledge infrastructure aligned with planetary regeneration.

**Migration Integration:** This plan now includes a complete strategy for integrating the existing regen-ai repository with its 18,824 successfully scraped documents into the KOI architecture through a three-repository structure: koi-sensors (transformed regen-ai), koi-processor (new), and GAIA (existing agents).

**Critical Insight:** The artifact chains we create (source → raw → normalized → markdown → enriched → embedding) ARE the knowledge graph foundation. This graph must be explicit from the start using Graphiti, not added later.

**Key Recommendations:**
1. **Dual Identification**: Use RIDs for semantic identity + CIDs for content deduplication
2. **Transformation Provenance**: Track every operation with Content-Addressable Transformations (CATs)
3. **Explicit Knowledge Graph**: Every RID creates a graph node, every transformation creates an edge (via Graphiti)
4. **Temporal Tracking**: Graphiti provides time-aware queries ("what did we know when?")
5. **Separation of Concerns**: KOI NetworkGraph for infrastructure, Knowledge Graph for content
6. **Cost-Aware Processing**: Smart optimization with local models where possible
7. **Phased Implementation**: Start simple (2 weeks MVP), evolve to full capabilities (12 weeks total)
8. **Migration First**: Begin by transforming existing infrastructure, preserving all scraped content

## 1. Understanding KOI

### What is KOI?

KOI (Knowledge Organization Infrastructure) is a distributed system for managing knowledge flows across heterogeneous networks, developed through collaborative research between BlockScience, Metagov, and RMIT University.

**Evolution:**
- **v1 (KMS)**: Monolithic knowledge management
- **v2 (KOI Pond)**: API-based with primitive sensors  
- **v3 (KOI-net)**: Network protocol with full/partial nodes ← *We'll use this*

**Core Concepts:**
- **RIDs (Resource Identifiers)**: Stable references that persist across system changes
- **FUN Events**: Forget, Update, New - driving state changes
- **Fractal Architecture**: Networks can act as single nodes
- **Manifests**: SHA-256 hashes tracking all transformations

## 2. Current GAIA Analysis

### What We Have
- ElizaOS agent framework (5 agents running)
- PostgreSQL with pgvector for embeddings
- Basic document chunking and RAG
- **18,824 documents successfully scraped and indexed** via regen-ai repository
  - 13,159 Notion pages (100% complete)
  - 4,680 website pages (100% complete)
  - 839 blog posts (100% complete)
  - 146 YouTube transcripts (73% complete)
  - Plus Twitter, Discord, Telegram content
- Sophisticated scraping infrastructure at `/home/regenai/project`
- Some KOI registry tables in plugin-knowledge-local

### What's Missing
- Real-time content monitoring
- Knowledge graph capabilities
- Multi-agent coordination
- Semantic entity extraction
- Transformation accountability
- Commons knowledge sharing

### Integration Opportunity
We have two major assets to leverage:
1. **regen-ai repository**: Mature scraping infrastructure with 18,824 documents
2. **plugin-knowledge-local**: KOI registry components already implemented

The strategy is to enhance, not replace, these working systems.

## 3. Repository Architecture & Node Distribution

### Three-Repository Strategy

We recommend separating concerns across three repositories, each containing specific types of KOI nodes:

```
┌────────────────────────────────────────────────────────────────┐
│                    REPOSITORY ARCHITECTURE                      │
├────────────────────────────────────────────────────────────────┤
│                                                                  │
│  gaiaaiagent/koi-sensors (RENAME from regen-ai)                │
│  └── SENSOR NODES (Partial Nodes)                              │
│      ├── Twitter Sensor Node                                   │
│      ├── Telegram Sensor Node                                  │
│      ├── Discord Sensor Node                                   │
│      ├── Notion Sensor Node (wraps existing scraper)          │
│      ├── Website Monitor Node (wraps existing crawler)         │
│      ├── Blog RSS Sensor Node                                  │
│      └── YouTube Sensor Node (wraps existing scraper)          │
│                                                                  │
│  gaiaaiagent/koi-processor (NEW)                               │
│  └── PROCESSOR & COORDINATOR NODES (Full Nodes)                │
│      ├── KOI Coordinator Node (port 8000)                      │
│      ├── Document Processor Node (port 8001)                   │
│      ├── Entity Extractor Node (port 8002)                     │
│      ├── Embedding Generator Node (port 8003)                  │
│      └── Knowledge Graph Builder Node (port 8004)              │
│                                                                  │
│  gaiaaiagent/GAIA (EXISTING)                                   │
│  └── AGENT CLIENT NODES (Partial Nodes)                        │
│      ├── RegenAI Agent Node                                    │
│      ├── Advocate Agent Node                                   │
│      ├── VoiceOfNature Agent Node                              │
│      ├── Governor Agent Node                                   │
│      └── Narrator Agent Node                                   │
│                                                                  │
└────────────────────────────────────────────────────────────────┘
```

### Node Communication Flow

```
                    KOI Event Flow
                    
Twitter Sensor ─┐
Telegram Sensor ─┤
Notion Sensor ───┼──NEW──► Coordinator ──ROUTE──► Processors
Website Sensor ──┤         (Full Node)            (Full Nodes)
YouTube Sensor ──┘              │
                                │
                           FUN Events
                                │
                                ▼
                         Agent Clients
                        (Partial Nodes)
```

### Why This Architecture?

1. **Separation of Concerns**: Each repository has a clear, single responsibility
2. **Independent Scaling**: Sensors, processors, and agents can scale independently
3. **Preserve Working Code**: Your existing scrapers become sensor nodes
4. **Clear Data Flow**: Unidirectional flow from sensors → processors → agents

## 4. Proposed System Architecture

### Integrated System Design

```
┌─────────────────────────────────────────────────────────────┐
│         EXISTING REGEN-AI → KOI-SENSORS (Real-time)         │
│  Twitter | Telegram | Discord | Blog | Podcast | Notion      │
│            (Wrapped as KOI Sensor Nodes)                     │
└────────────────────────────────┬────────────────────────────┘
                                 ▼
┌─────────────────────────────────────────────────────────────┐
│              KOI PROCESSOR NODE (Port 8100)                  │
│  ┌─────────────────────────────────────────────────────┐    │
│  │  • Dual ID Generation (RID + CID)                   │    │
│  │  • Modular Processing Pipeline                      │    │
│  │  • Transformation Provenance (CATs)                 │    │
│  │  • Cost Optimization                                │    │
│  └─────────────────────────────────────────────────────┘    │
└────────────────────────────────┬────────────────────────────┘
                                 ▼
┌─────────────────────────────────────────────────────────────┐
│                     STORAGE LAYER                            │
│  PostgreSQL + pgvector | Graphiti (Neo4j) | RDF Export      │
└────────────────────────────────┬────────────────────────────┘
                                 ▼
┌─────────────────────────────────────────────────────────────┐
│               ELIZAOS AGENTS (Port 3000)                     │
│  RegenAI | Advocate | VoiceOfNature | Governor | Narrator    │
│           (Each as lightweight KOI Partial Node)             │
└─────────────────────────────────────────────────────────────┘
```

### Key Design Decisions

#### 1. Dual Identification System

```typescript
interface DualIdentification {
  rid: string;  // WHERE it came from: "orn:regen.document:notion/page-123"
  cid: string;  // WHAT it contains: "cid:sha256:abc123..."
}
```

**Benefits:**
- RIDs provide stable semantic identity
- CIDs enable perfect deduplication
- Together: provenance + efficiency

#### 2. Transformation Provenance (CATs)

```typescript
interface TransformationReceipt {
  cat_id: string;              // Content-Addressable Transformation ID
  input_rid: string;           // Source RID
  output_rid: string;          // Result RID  
  input_cid: string;           // Source content hash
  output_cid: string;          // Result content hash
  recipe: {
    stage: string;             // "enrichment", "embedding", etc.
    model?: string;            // "gpt-4o-mini"
    prompt_template_cid?: string;  // Prompts stored as CIDs!
    parameters: any;           // Temperature, etc.
  };
  timestamp: Date;
  processing_time_ms: number;
  cost: number;                // Track API costs
  signature?: string;          // For verification
}
```

**Benefits:**
- Complete audit trail
- Reproducible transformations
- Cost tracking
- Debugging capability

#### 3. Separation of Concerns

**KOI Processor Node (Standalone Service):**
- Processes documents ONCE
- Generates RIDs and CIDs
- Tracks transformations
- Manages costs

**ElizaOS Agents (Lightweight Clients):**
- Fetch by RID/CID (instant)
- No heavy processing
- Poll for relevant events
- Start in seconds

## 5. Server Resource Validation

### Your Server Specifications
- **RAM**: 32GB
- **Storage**: Large capacity
- **Current Load**: 5 ElizaOS agents + PostgreSQL + existing services

### Memory Analysis: KOI Uses LESS Memory!

**Current System (without KOI):**
```
5 Agents (3GB each):     15GB  # Each loads full knowledge
PostgreSQL:               3GB
Other services:           2GB
-------------------
Total:                  ~20GB used / 32GB available
```

**With KOI Architecture:**
```
5 Agents (lightweight):   2.5GB  # Only 500MB each!
KOI Processor:           2GB
Neo4j/Graphiti:          6GB
Ollama:                  3GB
PostgreSQL:              3GB
Other services:          2GB
-------------------
Total:                 ~18.5GB used / 32GB available
```

### Performance Improvements

```yaml
Before KOI:
  Agent Startup: 5-10 minutes (loading 18,824 docs)
  Memory per Agent: 3GB
  Document Processing: Per agent (5x redundant)
  Total RAM Used: ~20GB

After KOI:
  Agent Startup: <10 seconds (fetch by RID/CID)
  Memory per Agent: 500MB
  Document Processing: Once (KOI Processor)
  Total RAM Used: ~18.5GB
```

**Conclusion**: Your 32GB server is MORE than adequate. KOI actually reduces memory usage while improving performance by 10x!

## 6. Technology Stack

| Component | Technology | Rationale |
|-----------|------------|-----------|
| **Document Storage** | PostgreSQL + pgvector | Already working, proven scale |
| **Knowledge Graph** | Graphiti (on Neo4j) | Temporal tracking, LLM-native |
| **Local Embeddings** | Ollama (nomic-embed-text) | Free, fast, private |
| **Text Generation** | OpenAI GPT-4o-mini | Cost-effective, quality |
| **Event System** | KOI-net protocol | Distributed coordination |
| **Export Format** | RDF/JSON-LD | Standards compliance |
| **Processing** | TypeScript/Bun | Fast, familiar |

### Why Graphiti?

After comparing Neo4j, RDF/SPARQL, and [Graphiti](https://github.com/getzep/graphiti):

**Graphiti Advantages:**
- **Temporal by default**: Tracks when facts were true
- **LLM-native**: Built-in entity extraction
- **Fact invalidation**: Knowledge can expire/update
- **Neo4j foundation**: Familiar tools
- **Python-first**: Easier than RDF

```python
# Example: Temporal knowledge tracking
await graphiti.add_episode({
    "content": fact.content,
    "valid_from": fact.discovered_at,
    "valid_until": fact.invalidated_at,  # Facts can expire!
    "confidence": fact.confidence_score
})
```

## 5. Implementation Components

### Core Component 1: Dual ID Manager

```typescript
class DualIDManager {
  generateRID(source: string, identifier: string): string {
    // Semantic identity
    return `orn:regen.${source}:${identifier}`;
  }
  
  async calculateCID(content: string): Promise<string> {
    // Content hash for deduplication
    const hash = createHash('sha256').update(content).digest('hex');
    return `cid:sha256:${hash}`;
  }
  
  async processDocument(doc: Document): Promise<DualIDResult> {
    const rid = this.generateRID(doc.source, doc.id);
    const cid = await this.calculateCID(doc.content);
    
    if (this.cidExists(cid)) {
      // Content already processed, just map new RID
      return { status: 'duplicate', rid, cid };
    }
    
    // New content, process it
    return { status: 'new', rid, cid };
  }
}
```

### Core Component 2: Modular Processing Pipeline

```typescript
class ProcessingPipeline {
  stages = [
    new ChunkingStage({ maxTokens: 1000 }),
    new ConditionalEnrichmentStage({ costOptimizer }),
    new EmbeddingStage({ provider: 'ollama' }),
    new EntityExtractionStage({ provider: 'graphiti' }),
    new DeduplicationStage()
  ];
  
  async process(doc: Document): Promise<ProcessingResult> {
    let current = doc;
    const receipts = [];
    
    for (const stage of this.stages) {
      const inputCid = await this.calculateCID(current);
      current = await stage.process(current);
      const outputCid = await this.calculateCID(current);
      
      // Record transformation
      const receipt = await this.recordTransformation({
        inputCid, outputCid, stage: stage.name
      });
      receipts.push(receipt);
    }
    
    return { final: current, receipts };
  }
}
```

### Core Component 3: Cost Optimizer

```typescript
class CostOptimizer {
  private budgets = {
    daily: 100,     // $100/day total
    enrichment: 50, // $50/day for enrichment
    embedding: 20   // $20/day for embeddings
  };
  
  async shouldProcess(task: ProcessingTask): Promise<Decision> {
    // Check budget
    if (this.spent.daily >= this.budgets.daily) {
      return { should: false, reason: 'Budget exceeded' };
    }
    
    // Skip code blocks for enrichment
    if (task.type === 'enrich' && this.isCode(task.content)) {
      return { should: false, reason: 'Skip code enrichment' };
    }
    
    // Skip tiny chunks
    if (task.tokens < 50) {
      return { should: false, reason: 'Too small' };
    }
    
    // Select model based on importance
    const model = task.priority > 0.8 ? 'gpt-4o-mini' : 'gpt-3.5-turbo';
    
    return { should: true, model };
  }
}
```

### Core Component 4: Incremental Processing

```typescript
class IncrementalProcessor {
  async processUpdate(doc: Document) {
    const existingCid = await this.getExistingCID(doc.rid);
    const newCid = await this.calculateCID(doc.content);
    
    if (existingCid === newCid) {
      return { status: 'unchanged' };
    }
    
    // Find what changed
    const diff = await this.diffContent(existingCid, newCid);
    
    // Process only changed chunks
    for (const change of diff.changes) {
      if (change.type === 'addition') {
        await this.processChunk(change.content);
      } else if (change.type === 'modification') {
        await this.reprocessChunk(change.content);
      } else if (change.type === 'deletion') {
        await this.markDeleted(change.chunkId);
      }
    }
    
    return {
      status: 'updated',
      version: doc.version + 1,
      saved_cost: this.calculateSavedCost(diff.unchanged)
    };
  }
}
```

### Core Component 5: KOI-Enhanced Agent

```typescript
class KOIEnhancedAgent {
  private koiClient: KOIClient;
  
  async initialize(runtime: IAgentRuntime) {
    // Lightweight KOI client instead of heavy processing
    this.koiClient = new KOIClient({
      endpoint: 'http://localhost:8100',
      agentId: runtime.agentId
    });
    
    // Poll for relevant events
    this.subscriptions = this.getAgentPatterns();
    setInterval(() => this.pollEvents(), 30000);
  }
  
  private getAgentPatterns(): string[] {
    // Subscribe based on agent personality
    if (this.character.name === 'Governor') {
      return ['orn:regen.governance:*', 'orn:regen.proposal:*'];
    } else if (this.character.name === 'VoiceOfNature') {
      return ['orn:regen.ecology:*', 'orn:regen.climate:*'];
    }
    // etc...
  }
  
  async query(text: string): Promise<any> {
    // Fetch pre-processed knowledge by RID/CID
    return await this.koiClient.query(text);
  }
}
```

## 6. Implementation Roadmap (With Migration & Knowledge Graph)

### Phase 1: MVP Foundation + Graph Infrastructure (Weeks 1-2)
**Goal:** Basic KOI infrastructure with dual identification + knowledge graph foundation

```bash
Week 1:
✅ Transform regen-ai → koi-sensors repository
✅ Set up Neo4j database instance
✅ Install and configure Graphiti
✅ Implement RID + CID generation with graph node creation
✅ Create transformation receipt system (CATs) as graph edges
✅ Wrap existing scrapers as sensor nodes

Week 2:
✅ Process 18,824 existing documents through KOI
✅ Populate knowledge graph with artifact chains
✅ Create retroactive CAT receipts in graph
✅ Test graph queries for transformation chains
✅ Create lightweight KOI plugin for ElizaOS
✅ Deploy to production with migrated content + graph
```

**Success Metrics:**
- All 18,824 documents migrated with graph nodes: Day 10
- Complete artifact chains in Neo4j: Day 12
- Graph query latency < 100ms
- First agent using KOI + graph: Day 7

### Phase 2: Processing Pipeline + Incremental Updates (Weeks 3-4)
**Goal:** Modular pipeline with cost optimization + file watchers

```bash
Week 3:
✅ Implement file watchers for /home/regenai/project/data
✅ Chunking + Enrichment stages
✅ Ollama embedding stage (free)
✅ Deduplication stage with CID tracking

Week 4:
✅ Cost optimizer implementation
✅ Incremental processing for new scraped content
✅ Performance testing with live updates
✅ Validate deduplication on 18,824 documents
```

**Success Metrics:**
- Daily costs < $100
- 30%+ storage reduction via deduplication
- Incremental updates < 1 minute
- Zero duplicate processing

### Phase 3: Entity Extraction & Graph Enhancement (Weeks 5-6)
**Goal:** Extract entities and relationships from content

```bash
Week 5:
✅ Configure Graphiti entity extraction
✅ Process markdown artifacts for entities
✅ Deduplicate entities across documents
✅ Create entity relationship graph

Week 6:
✅ Build GraphQL query interface
✅ Implement temporal queries
✅ Connect agents to graph queries
✅ Create graph visualization dashboard
```

**Success Metrics:**
- 10,000+ entities extracted and linked
- Entity deduplication >90% accurate
- Temporal queries working ("what did we know when?")
- GraphQL API response < 50ms

### Phase 4: Sensor Network (Weeks 7-8)
**Goal:** Real-time content monitoring

```bash
Week 7:
✅ Twitter sensor (priority)
✅ Telegram sensor
✅ Discord sensor

Week 8:
✅ RSS/Blog sensor
✅ Notion webhook sensor
✅ Integration testing
```

**Success Metrics:**
- < 1 minute detection latency
- 100% content coverage
- Auto RID generation

### Phase 5: Multi-Agent Coordination (Weeks 9-10)
**Goal:** All agents on KOI infrastructure with migration complete

```bash
Week 9:
✅ Migrate all 5 agents to KOI
✅ Deploy KOI client plugin to each agent
✅ Implement event subscriptions
✅ Update agent character files with KOI settings

Week 10:
✅ Test multi-agent scenarios
✅ Performance optimization
✅ Validate all agents accessing migrated knowledge
✅ Full system migration validation
```

**Success Metrics:**
- All 5 agents using KOI successfully
- Agent startup < 10 seconds
- Coordinated responses working
- 50%+ query performance improvement

### Phase 6: Commons Protocol (Weeks 11-12)
**Goal:** Enable partner knowledge sharing

```bash
Week 11:
✅ RDF ontology design
✅ JSON-LD exporter
✅ SPARQL endpoint

Week 12:
✅ Permission system
✅ Federation protocol
✅ Partner testing
```

**Success Metrics:**
- RDF export working
- 1+ partner connected
- Federated queries successful

## 7. Deployment Configuration

### Docker Compose Setup

```yaml
version: '3.8'

services:
  koi-processor:
    build: ./koi-processor
    ports:
      - "8100:8100"
    environment:
      - POSTGRES_URL=postgresql://postgres:postgres@postgres:5432/koi
      - OLLAMA_URL=http://ollama:11434
      - NEO4J_URI=bolt://neo4j:7687
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - postgres
      - ollama
      - neo4j

  postgres:
    image: pgvector/pgvector:pg16
    environment:
      - POSTGRES_DB=koi
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data

  neo4j:
    image: neo4j:5-community
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/regennetwork
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
    volumes:
      - neo4j_data:/data

  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

volumes:
  postgres_data:
  neo4j_data:
  ollama_data:
```

### Agent Configuration Update

```json
{
  "name": "Governor",
  "plugins": [
    "@elizaos/plugin-knowledge-koi"  // New lightweight plugin
  ],
  "settings": {
    "KOI_PROCESSOR_URL": "http://localhost:8100",
    "KOI_POLL_INTERVAL": 30000,
    "KNOWLEDGE_FILTER": "governance"  // Agent-specific filter
  }
}
```

## 8. Success Metrics & KPIs

### Technical Metrics
- **Content ingestion**: 1000+ items/day
- **Processing latency**: < 5 seconds
- **Knowledge graph**: 100,000+ entities
- **Query response**: < 100ms
- **System uptime**: 99.9%
- **Deduplication ratio**: 10:1
- **Daily cost**: < $100

### Business Metrics
- **Agent response quality**: 90% relevance
- **Knowledge reuse**: 50% from cache
- **Commons participation**: 10+ organizations
- **Developer adoption**: 5+ integrations
- **Content coverage**: 100% of sources

## 9. Risk Mitigation

| Risk | Mitigation |
|------|------------|
| **Complexity** | Start with MVP, add features incrementally |
| **Cost overrun** | Strict budget controls, local models |
| **Learning curve** | Comprehensive documentation, training |
| **Integration issues** | Backward compatibility, fallback mode |
| **Data consistency** | Manifests, reconciliation protocols |

## 10. Critical Success Factors

### 1. Start Simple
Begin with RID/CID and basic processing. Add advanced features gradually.

### 2. Maintain Compatibility
```typescript
class BackwardCompatiblePlugin {
  async query(text: string) {
    try {
      return await this.koiClient.query(text);  // New system
    } catch {
      return await this.legacyQuery(text);      // Fallback
    }
  }
}
```

### 3. Track Everything
Every transformation must have a CAT receipt. No exceptions.

### 4. Optimize Costs
- Use Ollama for embeddings (free)
- Skip enrichment for code
- Cache aggressively
- Batch API calls

### 5. Test Early
Load test with 10,000 documents in week 2, not week 12.

## 11. Key Innovations

### Our Unique Contributions

1. **Dual Identification Pattern**: RID + CID solving both semantic identity and deduplication
2. **CAT Receipts**: Complete transformation provenance with cost tracking
3. **Hybrid Graph Strategy**: Graphiti now, RDF export for commons later
4. **Cost-Aware Pipeline**: Smart model selection based on content value
5. **Incremental Processing**: Only process what changed

### Why This Architecture Wins

- **Scalable**: Fractal architecture, unlimited growth
- **Efficient**: Process once, use everywhere
- **Accountable**: Complete audit trail via CATs
- **Flexible**: Modular pipeline, easy to extend
- **Future-proof**: Commons-ready from day one

## 12. Next Steps

### Immediate Actions (This Week)

1. **Day 1-2**: Set up KOI Processor Node repository
2. **Day 3-4**: Implement dual ID system
3. **Day 5**: Create first transformation receipt

### Quick Validation

```bash
# Test dual identification
curl -X POST http://localhost:8100/process \
  -H 'Content-Type: application/json' \
  -d '{"source":"test","id":"1","content":"Test document"}'

# Expected response:
{
  "rid": "orn:regen.test:1",
  "cid": "cid:sha256:abc123...",
  "status": "processed",
  "receipts": [...]
}
```

## 13. Migration Strategy

### Overview

Migrating the existing regen-ai infrastructure (18,824 documents) to KOI architecture requires a phased approach that preserves existing work while introducing new capabilities.

### Phase 1: Repository Transformation (Week 1)

#### Transform regen-ai → koi-sensors

```bash
# Rename and restructure repository
git clone https://github.com/gaiaaiagent/regen-ai koi-sensors
cd koi-sensors
git remote set-url origin https://github.com/gaiaaiagent/koi-sensors

# Wrap existing scrapers as sensor nodes
mkdir -p src/sensors/{notion,medium,twitter,discourse}
mv existing-scrapers/* src/sensors/
```

#### Sensor Node Wrapper Pattern

```typescript
// src/sensors/notion/index.ts
import { existingNotionScraper } from './legacy-scraper';
import { SensorNode } from '@koi/sensor-framework';

export class NotionSensorNode extends SensorNode {
  async sense(): Promise<SensorEvent[]> {
    // Leverage existing scraper
    const documents = await existingNotionScraper.scrape();
    
    // Transform to KOI events
    return documents.map(doc => ({
      type: 'NEW',
      rid: this.generateRID(doc),
      cid: await this.computeCID(doc.content),
      content: doc,
      source: 'notion',
      timestamp: Date.now()
    }));
  }
}
```

### Phase 2: Document Migration (Week 2)

#### Process Existing 18,824 Documents

```typescript
// migration/process-existing.ts
import { KOIProcessor } from '@koi/processor';
import fs from 'fs-extra';
import path from 'path';

async function migrateExistingDocuments() {
  const processor = new KOIProcessor();
  const dataDir = '/home/regenai/project/data';
  
  // Track progress
  let processed = 0;
  const total = 18824;
  
  // Process by source type for better RID assignment
  const sources = ['notion', 'medium', 'twitter', 'discourse'];
  
  for (const source of sources) {
    const sourceDir = path.join(dataDir, source);
    const files = await fs.readdir(sourceDir);
    
    for (const file of files) {
      const content = await fs.readFile(
        path.join(sourceDir, file), 
        'utf-8'
      );
      
      // Generate dual identification
      const rid = `orn:regen.document:${source}/${path.basename(file, '.json')}`;
      const cid = await processor.computeCID(content);
      
      // Check if already processed (deduplication)
      if (!await processor.exists(cid)) {
        await processor.process({
          rid,
          cid,
          content: JSON.parse(content),
          source,
          originalPath: path.join(sourceDir, file)
        });
      }
      
      processed++;
      if (processed % 100 === 0) {
        console.log(`Progress: ${processed}/${total} (${Math.round(processed/total*100)}%)`);
      }
    }
  }
}
```

### Phase 3: Incremental Updates (Week 3)

#### File Watcher for New Content

```typescript
// src/watchers/file-watcher.ts
import chokidar from 'chokidar';
import { KOIProcessor } from '@koi/processor';

export class IncrementalWatcher {
  private processor: KOIProcessor;
  private watcher: chokidar.FSWatcher;
  
  constructor() {
    this.processor = new KOIProcessor();
    this.watcher = chokidar.watch('/home/regenai/project/data', {
      persistent: true,
      ignoreInitial: true,
      awaitWriteFinish: {
        stabilityThreshold: 2000,
        pollInterval: 100
      }
    });
  }
  
  start() {
    this.watcher
      .on('add', path => this.processNewFile(path))
      .on('change', path => this.processUpdate(path))
      .on('unlink', path => this.processRemoval(path));
  }
  
  private async processNewFile(filePath: string) {
    const content = await fs.readFile(filePath, 'utf-8');
    const source = this.extractSource(filePath);
    
    // Generate identifiers
    const rid = this.generateRID(filePath, source);
    const cid = await this.processor.computeCID(content);
    
    // Process through KOI
    await this.processor.process({
      type: 'NEW',
      rid,
      cid,
      content: JSON.parse(content),
      source
    });
    
    console.log(`Processed new document: ${rid}`);
  }
}
```

### Phase 4: Agent Migration (Week 4)

#### Update Agent Configurations

```typescript
// Update each agent's character file
{
  "name": "RegenAI",
  "plugins": [
    "@elizaos/plugin-bootstrap",
    "@elizaos/plugin-sql",
    "@elizaos/plugin-knowledge",
    "@koi/plugin-client"  // Add KOI client plugin
  ],
  "settings": {
    "KNOWLEDGE_PATH": "./knowledge",
    "KOI_PROCESSOR_URL": "http://localhost:8000",
    "KOI_AGENT_RID": "orn:regen.agent:regenai"
  }
}
```

#### KOI Client Plugin for Agents

```typescript
// packages/plugin-koi-client/src/index.ts
export class KOIClientPlugin implements Plugin {
  async onLoad(runtime: Runtime) {
    // Connect to KOI Processor
    const processor = new KOIClient(
      runtime.getSetting('KOI_PROCESSOR_URL')
    );
    
    // Register agent with its RID
    await processor.registerAgent({
      rid: runtime.getSetting('KOI_AGENT_RID'),
      capabilities: ['query', 'transform', 'reason']
    });
    
    // Subscribe to relevant events
    processor.subscribe('NEW', async (event) => {
      if (this.isRelevant(event, runtime)) {
        await runtime.processKnowledge(event);
      }
    });
  }
}
```

### Phase 5: Deduplication & Optimization (Week 5)

#### Content Deduplication System

```typescript
// src/deduplication/index.ts
export class DeduplicationService {
  private cidIndex: Map<string, string[]>; // CID -> [RIDs]
  
  async deduplicate(documents: Document[]): Promise<Document[]> {
    const unique = new Map<string, Document>();
    
    for (const doc of documents) {
      const cid = await this.computeCID(doc.content);
      
      if (!unique.has(cid)) {
        unique.set(cid, doc);
        // Track all RIDs that map to this CID
        this.cidIndex.set(cid, [doc.rid]);
      } else {
        // Document with same content exists
        // Just add the RID mapping
        this.cidIndex.get(cid)!.push(doc.rid);
      }
    }
    
    console.log(`Deduplication: ${documents.length} → ${unique.size} unique`);
    return Array.from(unique.values());
  }
}
```

### Migration Timeline

| Week | Task | Status |
|------|------|--------|
| 1 | Repository transformation (regen-ai → koi-sensors) | Ready |
| 1 | Wrap existing scrapers as sensor nodes | Ready |
| 2 | Process 18,824 existing documents | Ready |
| 2 | Generate RIDs and CIDs for all content | Ready |
| 3 | Implement file watchers for incremental updates | Ready |
| 3 | Test deduplication system | Ready |
| 4 | Update agent configurations | Ready |
| 4 | Deploy KOI client plugin | Ready |
| 5 | Full system validation | Ready |
| 5 | Performance optimization | Ready |

### Risk Mitigation

1. **Data Loss Prevention**
   - Keep original regen-ai repo as backup
   - Process documents in batches with checkpoints
   - Maintain mapping of old paths to new RIDs

2. **Downtime Minimization**
   - Run KOI processor in parallel during migration
   - Agents continue using existing knowledge
   - Switch over once migration validated

3. **Rollback Strategy**
   - Keep existing infrastructure running
   - Maintain compatibility layer
   - Document all RID/CID mappings

### Success Metrics

- ✅ All 18,824 documents migrated with RIDs and CIDs
- ✅ Zero data loss during migration
- ✅ Deduplication reduces storage by >30%
- ✅ Query performance improves by >50%
- ✅ All 5 agents successfully using KOI
- ✅ Incremental updates processing in <1 minute

## 14. Knowledge Graph Architecture

### Critical Distinction: Two Graphs

KOI provides a **NetworkGraph** for infrastructure topology, but we need a separate **Knowledge Graph** for content:

```
KOI NetworkGraph (Infrastructure):
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│Twitter Node │----->│ Coordinator │----->│  Processor  │
└─────────────┘      └─────────────┘      └─────────────┘

Knowledge Graph (Content):
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│Tweet Source │-FETCH->│Raw Artifact│-NORM->│  Markdown  │
└─────────────┘      └─────────────┘      └─────────────┘
                            │
                        CONTAINS
                            ↓
                     ┌─────────────┐
                     │Person:Greg L│
                     └─────────────┘
```

### Graphiti as Our Knowledge Graph

Graphiti provides temporal knowledge graph capabilities perfect for KOI:

1. **Temporal Nodes**: Every artifact has valid_from/valid_to timestamps
2. **Transformation Edges**: CAT receipts become graph relationships
3. **Entity Extraction**: Automatic identification of people, orgs, concepts
4. **Episode Tracking**: Each transformation is a trackable episode
5. **Importance Scoring**: Prioritize high-value transformations

### Graph Schema

```cypher
// Core Artifact Nodes
(:Source {
  rid: String,        // orn:regen.source:twitter.com/status/123
  url: String,        // https://twitter.com/status/123
  type: String,       // 'url' | 'api' | 'database'
  accessible: Boolean,
  last_verified: DateTime
})

(:Artifact {
  rid: String,        // orn:regen.raw:twitter/123
  cid: String,        // cid:sha256:abc123...
  format: String,     // 'json' | 'markdown' | 'html'
  stage: String,      // 'raw' | 'normalized' | 'enriched'
  size_bytes: Integer,
  created_at: DateTime,
  valid_from: DateTime,
  valid_to: DateTime   // null if current
})

// Transformation Relationships
(:Artifact)-[:TRANSFORMED {
  cat: String,         // cat:normalize:abc123
  operation: String,   // 'fetch' | 'normalize' | 'convert'
  timestamp: DateTime,
  agent: String,       // 'twitter-sensor-v1'
  cost_tokens: Integer,
  cost_compute: Float,
  retroactive: Boolean,
  input_cid: String,
  output_cid: String
}]->(:Artifact)

// Entity Nodes (extracted by Graphiti)
(:Entity {
  type: String,        // 'Person' | 'Organization' | 'Concept'
  name: String,
  aliases: [String],
  first_seen: DateTime,
  importance_score: Float
})

// Content Relationships
(:Artifact)-[:CONTAINS]->(:Entity)
(:Artifact)-[:MENTIONS]->(:Entity)
(:Entity)-[:RELATED_TO {weight: Float}]->(:Entity)
```

### Artifact Chain as Graph

Complete transformation chain for a tweet:

```cypher
// Level 0: Source Reference
(source:Source {
  rid: 'orn:regen.source:twitter.com/RegenNetwork/status/123',
  url: 'https://twitter.com/RegenNetwork/status/123',
  type: 'url'
})

// Level 1: Raw Capture
(raw:Artifact {
  rid: 'orn:regen.raw:twitter/123',
  cid: 'cid:sha256:raw123...',
  format: 'json',
  stage: 'raw'
})

// Level 2: Normalized
(norm:Artifact {
  rid: 'orn:regen.normalized:twitter/123',
  cid: 'cid:sha256:norm456...',
  format: 'json',
  stage: 'normalized'
})

// Level 3: Markdown
(md:Artifact {
  rid: 'orn:regen.markdown:twitter/123',
  cid: 'cid:sha256:md789...',
  format: 'markdown',
  stage: 'markdown'
})

// Transformation Chain
(source)-[:FETCHED {cat: 'cat:fetch:001'}]->(raw)
(raw)-[:NORMALIZED {cat: 'cat:norm:002'}]->(norm)
(norm)-[:CONVERTED {cat: 'cat:md:003'}]->(md)

// Extracted Entities
(person:Entity {type: 'Person', name: 'Gregory Landua'})
(org:Entity {type: 'Organization', name: 'Regen Network'})

(md)-[:MENTIONS]->(person)
(md)-[:MENTIONS]->(org)
(person)-[:AFFILIATED_WITH]->(org)
```

## 15. Artifact Chain Implementation

### Complete Chain Structure

Every piece of content follows this 6-level transformation chain:

#### Level 0: Source Reference
- **Purpose**: Point to the actual external resource
- **RID Pattern**: `orn:regen.source:{domain}/{path}`
- **Storage**: Reference only (no content)
- **Examples**:
  - `orn:regen.source:twitter.com/RegenNetwork/status/123`
  - `orn:regen.source:notion.so/page-abc-def`
  - `orn:regen.source:github.com/regen-network/regen-ledger`

#### Level 1: Raw Artifact
- **Purpose**: First capture of external content
- **RID Pattern**: `orn:regen.raw:{source}/{id}`
- **Storage**: Original API response or HTML
- **CID**: Hash of raw response
- **Examples**:
  - Twitter API JSON response
  - Notion API page object
  - GitHub repository metadata

#### Level 2: Normalized Artifact
- **Purpose**: Standardized structure across sources
- **RID Pattern**: `orn:regen.normalized:{source}/{id}`
- **Storage**: Clean JSON with consistent fields
- **CID**: Hash of normalized JSON
- **Standard Fields**:
  ```json
  {
    "id": "string",
    "content": "string",
    "author": "string",
    "timestamp": "ISO8601",
    "source": "string",
    "metadata": {}
  }
  ```

#### Level 3: Markdown Artifact
- **Purpose**: Human and agent-readable format
- **RID Pattern**: `orn:regen.markdown:{source}/{id}`
- **Storage**: Markdown text
- **CID**: Hash of markdown
- **Features**: Headers, lists, links, code blocks

#### Level 4: Enriched Artifact
- **Purpose**: Add analytical insights
- **RID Pattern**: `orn:regen.enriched:{source}/{id}`
- **Storage**: Original + enrichments
- **CID**: Hash of enriched content
- **Enrichments**:
  - Sentiment analysis
  - Topic extraction
  - Entity recognition
  - Summary generation

#### Level 5: Embedding Artifact
- **Purpose**: Enable semantic search
- **RID Pattern**: `orn:regen.embedding:{source}/{id}`
- **Storage**: Vector array
- **CID**: Hash of embedding vector
- **Specifications**:
  - Model: text-embedding-3-small
  - Dimensions: 1536
  - Format: Float32 array

### CAT Receipt Chain

Each transformation generates a CAT receipt:

```typescript
interface CATReceipt {
  cat: string;           // Unique CAT ID
  operation: string;     // What transformation
  timestamp: number;     // When it happened
  
  input: {
    rid: string;         // Input artifact RID
    cid: string;         // Input content hash
  };
  
  output: {
    rid: string;         // Output artifact RID
    cid: string;         // Output content hash
  };
  
  agent: string;         // Who performed it
  cost?: {
    tokens?: number;     // LLM tokens used
    compute?: number;    // CPU/GPU seconds
    storage?: number;    // Bytes stored
  };
  
  retroactive: boolean;  // Was this reconstructed?
  verified: boolean;     // Has this been validated?
}
```

### Implementation for Each Source

#### Twitter Implementation
```typescript
// Source reference
sourceRID: "orn:regen.source:twitter.com/RegenNetwork/status/{id}"

// Transformation chain
fetch    → "orn:regen.raw:twitter/{id}"        // API response
normalize → "orn:regen.normalized:twitter/{id}" // Clean JSON
markdown  → "orn:regen.markdown:twitter/{id}"   // Tweet as MD
enrich    → "orn:regen.enriched:twitter/{id}"   // + sentiment
embed     → "orn:regen.embedding:twitter/{id}"  // Vector
```

#### Notion Implementation
```typescript
// Source reference
sourceRID: "orn:regen.source:notion.so/{page-id}"

// Transformation chain
fetch    → "orn:regen.raw:notion/{page-id}"        // API response
normalize → "orn:regen.normalized:notion/{page-id}" // Blocks→JSON
markdown  → "orn:regen.markdown:notion/{page-id}"   // Blocks→MD
enrich    → "orn:regen.enriched:notion/{page-id}"   // + topics
embed     → "orn:regen.embedding:notion/{page-id}"  // Vector
```

## 16. Graphiti Integration Checklist

### Phase 1: Infrastructure Setup
- [ ] **Install Neo4j Database**
  ```bash
  docker run -d \
    --name neo4j \
    -p 7474:7474 -p 7687:7687 \
    -e NEO4J_AUTH=neo4j/password \
    -v $PWD/neo4j/data:/data \
    neo4j:latest
  ```

- [ ] **Install Graphiti**
  ```bash
  pip install graphiti-core
  ```

- [ ] **Configure Graphiti Connection**
  ```python
  from graphiti_core import Graphiti
  
  graphiti = Graphiti(
    neo4j_uri="bolt://localhost:7687",
    neo4j_user="neo4j",
    neo4j_password="password"
  )
  ```

### Phase 2: Schema Creation
- [ ] **Create Artifact Node Types**
  - Source nodes (external references)
  - Artifact nodes (our copies)
  - Entity nodes (extracted concepts)

- [ ] **Create Transformation Relationships**
  - FETCHED (source → raw)
  - NORMALIZED (raw → normalized)
  - CONVERTED (normalized → markdown)
  - ENRICHED (markdown → enriched)
  - EMBEDDED (enriched → embedding)

- [ ] **Create Entity Relationships**
  - CONTAINS (artifact → entity)
  - MENTIONS (artifact → entity)
  - RELATED_TO (entity → entity)

### Phase 3: Migration Integration
- [ ] **Modify Migration Script**
  - Add Graphiti client initialization
  - Create graph node for each RID
  - Create graph edge for each transformation
  - Store CAT receipts as edge properties

- [ ] **Implement Batch Processing**
  ```python
  # Process in batches of 100
  for batch in chunks(artifacts, 100):
    with graphiti.batch() as batch_writer:
      for artifact in batch:
        batch_writer.add_node(artifact)
        batch_writer.add_edges(transformations)
  ```

- [ ] **Add Progress Tracking**
  - Log nodes created
  - Log edges created
  - Track failed operations
  - Generate summary report

### Phase 4: Entity Extraction
- [ ] **Configure Graphiti Entity Extraction**
  ```python
  graphiti.configure_extraction({
    'extract_entities': True,
    'entity_types': ['Person', 'Organization', 'Location', 'Concept'],
    'extract_relationships': True,
    'confidence_threshold': 0.7
  })
  ```

- [ ] **Process Documents for Entities**
  - Run extraction on markdown artifacts
  - Deduplicate entities across documents
  - Create entity relationships

### Phase 5: Temporal Features
- [ ] **Enable Temporal Tracking**
  ```python
  graphiti.enable_temporal({
    'track_changes': True,
    'maintain_history': True,
    'episodic_memory': True
  })
  ```

- [ ] **Implement Time-Based Queries**
  ```cypher
  // What did we know about climate on date X?
  MATCH (a:Artifact)-[:CONTAINS]->(e:Entity)
  WHERE e.name = 'climate change'
  AND a.valid_from <= datetime('2024-01-01')
  AND (a.valid_to IS NULL OR a.valid_to > datetime('2024-01-01'))
  RETURN a, e
  ```

### Phase 6: Query Interface
- [ ] **Create Common Queries**
  - Find transformation chain for document
  - Get all artifacts mentioning entity
  - Track document through pipeline
  - Find related entities

- [ ] **Build GraphQL API**
  ```graphql
  query GetArtifactChain($sourceRid: String!) {
    artifactChain(sourceRid: $sourceRid) {
      source { rid, url }
      transformations {
        operation
        timestamp
        input { rid, cid }
        output { rid, cid }
      }
    }
  }
  ```

### Phase 7: Monitoring & Optimization
- [ ] **Create Dashboard**
  - Total nodes by type
  - Transformations per day
  - Entity extraction statistics
  - Query performance metrics

- [ ] **Optimize Indices**
  ```cypher
  CREATE INDEX artifact_rid FOR (a:Artifact) ON (a.rid);
  CREATE INDEX artifact_cid FOR (a:Artifact) ON (a.cid);
  CREATE INDEX entity_name FOR (e:Entity) ON (e.name);
  ```

### Phase 8: RDF Export
- [ ] **Implement RDF Mapping**
  ```python
  def artifact_to_rdf(artifact):
    return {
      '@id': artifact.rid,
      '@type': 'koi:Artifact',
      'koi:contentHash': artifact.cid,
      'dc:format': artifact.format,
      'dc:created': artifact.created_at
    }
  ```

- [ ] **Create SPARQL Endpoint**
  - Export graph to RDF
  - Set up Fuseki or similar
  - Enable federated queries

## 17. Data Flow Architecture

### Real-Time Flow (New Content)

```
1. DETECTION
   ┌─────────────┐
   │Sensor Detects│ (Twitter API, Notion webhook, etc.)
   │ New Content  │
   └──────┬──────┘
          │
2. CAPTURE & IDENTIFY
   ┌──────▼──────┐
   │Create Source │ RID: orn:regen.source:twitter.com/status/123
   │  Reference   │ (No content, just pointer)
   └──────┬──────┘
          │
   ┌──────▼──────┐
   │ Fetch Raw    │ RID: orn:regen.raw:twitter/123
   │   Content    │ CID: cid:sha256:abc... (hash of API response)
   └──────┬──────┘
          │
3. GRAPH CREATION (Parallel)
   ┌──────┴──────┬──────────┐
   │             │          │
   ▼             ▼          ▼
[Neo4j Node] [CAT Receipt] [Event to KOI]
   │             │          │
   └──────┬──────┴──────────┘
          │
4. TRANSFORMATION PIPELINE
   ┌──────▼──────┐
   │  Normalize   │ RID: orn:regen.normalized:twitter/123
   │   Content    │ CID: cid:sha256:def...
   └──────┬──────┘
          │ → Graph Edge + CAT
   ┌──────▼──────┐
   │ Convert to   │ RID: orn:regen.markdown:twitter/123
   │   Markdown   │ CID: cid:sha256:ghi...
   └──────┬──────┘
          │ → Graph Edge + CAT
   ┌──────▼──────┐
   │   Enrich     │ RID: orn:regen.enriched:twitter/123
   │   Content    │ CID: cid:sha256:jkl...
   └──────┬──────┘
          │ → Graph Edge + CAT
   ┌──────▼──────┐
   │  Generate    │ RID: orn:regen.embedding:twitter/123
   │  Embedding   │ CID: cid:sha256:mno...
   └──────┬──────┘
          │
5. KNOWLEDGE READY
   ┌──────▼──────┐
   │Available for │ Query via Graphiti API
   │   Agents     │ Traverse transformation chain
   └─────────────┘
```

### Retroactive Flow (Existing 18,824 Documents)

```
1. INVENTORY
   ┌─────────────┐
   │Scan Existing │ /data/twitter/*.json
   │   Files      │ /data/notion/*.md
   └──────┬──────┘
          │
2. RECONSTRUCT CHAIN
   ┌──────▼──────┐
   │Create Source │ RID: orn:regen.source:twitter.com/status/123
   │  Reference   │ (Retroactive, based on file metadata)
   └──────┬──────┘
          │
   ┌──────▼──────┐
   │ Find/Create  │ If have: Use actual CID
   │Raw Artifact  │ If not: Use "cid:unknown:retroactive"
   └──────┬──────┘
          │
3. GRAPH POPULATION
   ┌──────▼──────┐
   │Create Nodes  │ Batch insert to Neo4j
   │Create Edges  │ Mark as retroactive=true
   │  Add CATs    │ Approximate timestamps
   └──────┬──────┘
          │
4. FILL GAPS
   ┌──────▼──────┐
   │  Identify    │ What artifacts are missing?
   │Missing Steps │ Can we regenerate them?
   └─────────────┘
```

### Parallel Infrastructure vs Content Graphs

```
INFRASTRUCTURE GRAPH (KOI NetworkGraph)
========================================
TwitterSensor ──connected──> Coordinator
Coordinator ──connected──> Processor
Processor ──connected──> AgentNode

Purpose: Track which nodes can talk to which
Query: "What sensors feed this processor?"


CONTENT GRAPH (Graphiti Knowledge Graph)
========================================
Tweet123 ──FETCHED──> RawJSON ──NORMALIZED──> CleanJSON
CleanJSON ──CONVERTED──> Markdown ──ENRICHED──> Enhanced
Enhanced ──EMBEDDED──> Vector

Purpose: Track content transformations
Query: "Show me all transformations of this tweet"
```

### Event Flow Integration

```
Sensor Event → KOI Processor → Graph Writer
     │              │              │
     │              │              └─> Neo4j
     │              └─> PostgreSQL
     └─> Event Bus → Agents
```

## Conclusion

This KOI-enhanced architecture transforms GAIA from a basic RAG system into a sophisticated knowledge infrastructure that:

1. **Processes content once** with complete provenance
2. **Enables real-time monitoring** via sensor nodes
3. **Builds temporal knowledge graphs** with Graphiti
4. **Coordinates multiple agents** through events
5. **Prepares for commons sharing** with RDF export

The phased implementation reduces risk while delivering value quickly. Starting with the dual identification MVP (2 weeks), we progressively add capabilities to reach full commons-ready infrastructure (12 weeks).

By combining KOI's distributed architecture with practical optimizations (cost management, incremental processing, backward compatibility), we create a system that's both powerful and sustainable.

---

*Version: 5.0 - Complete with Knowledge Graph Architecture*
*Date: 2024*
*Status: Ready for Implementation*
*Key Additions:*
- *Explicit knowledge graph architecture using Graphiti*
- *Complete artifact chain model (source → raw → normalized → markdown → enriched → embedding)*
- *Integration strategy for 18,824 existing documents*
- *Detailed implementation checklists with graph integration*

## Appendix: Quick Reference

### RID Namespace Convention
```
orn:regen.{type}:{identifier}

Types:
- document, twitter, telegram, discord, blog, podcast
- person, org, project, location, concept
- claim, evidence, question
- agent, response, transformation
```

### Cost Model
```
GPT-4o: $0.03/1K tokens
GPT-4o-mini: $0.002/1K tokens
GPT-3.5-turbo: $0.001/1K tokens
Ollama: $0 (local)
```

### Key Repositories
- [BlockScience/koi-net](https://github.com/BlockScience/koi-net)
- [BlockScience/rid-lib](https://github.com/BlockScience/rid-lib)
- [getzep/graphiti](https://github.com/getzep/graphiti)

### Contact Points
- GAIA AI: /opt/projects/GAIA
- KOI Processor: http://localhost:8100
- Neo4j Browser: http://localhost:7474
- ElizaOS Agents: http://localhost:3000